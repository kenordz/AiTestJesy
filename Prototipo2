import os
import sys
import json
import base64
import threading
import queue
import time
import asyncio
import random  # Importamos random para la selección aleatoria de frases
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import FileResponse
from dotenv import load_dotenv
import httpx
import openai
from google.cloud import speech
from twilio.rest import Client
from twilio.twiml.voice_response import VoiceResponse
import uvicorn
import logging
from pydub import AudioSegment  # Importamos pydub para manejar archivos de audio
import string  # Para limpiar la puntuación de las transcripciones

# Configuración de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cargar variables de entorno desde el archivo .env
load_dotenv()

# Configurar credenciales (no eliminar)
required_env_vars = [
    "GOOGLE_APPLICATION_CREDENTIALS",
    "ELEVENLABS_API_KEY",
    "VOICE_ID",
    "TWILIO_ACCOUNT_SID",
    "TWILIO_AUTH_TOKEN",
    "OPENAI_API_KEY",
    "NGROK_URL",
    "TWIML_BIN_URL",
]

missing_vars = [var for var in required_env_vars if not os.getenv(var)]
if missing_vars:
    logger.error(
        f"Las siguientes variables de entorno no están configuradas correctamente: {', '.join(missing_vars)}"
    )
    sys.exit(1)

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
VOICE_ID = os.getenv("VOICE_ID")
TWILIO_ACCOUNT_SID = os.getenv("TWILIO_ACCOUNT_SID")
TWILIO_AUTH_TOKEN = os.getenv("TWILIO_AUTH_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NGROK_URL = os.getenv("NGROK_URL").rstrip('/')
TWIML_BIN_URL = os.getenv("TWIML_BIN_URL")

openai.api_key = OPENAI_API_KEY

app = FastAPI()
twilio_client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)

# Parámetros para Eleven Labs (no eliminar)
ELEVENLABS_STABILITY = 0.1  # Más expresividad (original)
ELEVENLABS_SIMILARITY_BOOST = 0.9  # Mayor consistencia con el tono base (original)

MAX_CHAR_LIMIT = 300  # Límite de caracteres por fragmento
MAX_RETRIES = 3

STATIC_AUDIO_PATH = "static"
ERROR_AUDIO_PATH = os.path.join(STATIC_AUDIO_PATH, "error.mp3")

# Mensajes estáticos existentes
STATIC_MESSAGES = {
    "welcome": "Thank you for connecting to NetConnect Services, how can I help you today?",
    "wait": [
        "Ok, give me one second, please.",
        "Ok, one moment, please.",
        "Hold on, please.",
        "Just a moment, please.",
        "Alright, let me check that for you, one second."
    ],
    "error": "We are experiencing technical difficulties. Please try again later.",
}

# (1) Lista de frases de despedida:
# Se agregan las frases exactas que deben activar la despedida.
# Aseguramos que "good evening", "good night", etc. no estén aquí para evitar falsos positivos.
farewell_phrases = [
    "goodbye", 
    "bye", 
    "see you", 
    "thank you very much", 
    "that's all i needed"
]

async def generate_static_audio():
    """Genera los archivos de audio para los mensajes estáticos si no existen."""
    if not os.path.exists(STATIC_AUDIO_PATH):
        os.makedirs(STATIC_AUDIO_PATH)

    for key, text in STATIC_MESSAGES.items():
        if key == 'wait':
            for idx, phrase in enumerate(text):
                audio_filename = f"{key}_{idx}.mp3"
                audio_path = os.path.join(STATIC_AUDIO_PATH, audio_filename)
                if not os.path.exists(audio_path):
                    await generate_audio_file(phrase, audio_path)
                    logger.info(f"Audio estático generado: {audio_path}")
                else:
                    logger.info(f"Audio estático ya existe: {audio_path}")
        else:
            audio_filename = f"{key}.mp3"
            audio_path = os.path.join(STATIC_AUDIO_PATH, audio_filename)
            if not os.path.exists(audio_path):
                await generate_audio_file(text, audio_path)
                logger.info(f"Audio estático generado: {audio_path}")
            else:
                logger.info(f"Audio estático ya existe: {audio_path}")

    # (2) Comprobar si existe el archivo de despedida (goodbye.mp3)
    # Si no existe, se registra en el log.
    goodbye_audio_path = os.path.join(STATIC_AUDIO_PATH, "goodbye.mp3")
    if not os.path.exists(goodbye_audio_path):
        logger.warning("No existe el archivo goodbye.mp3. Las despedidas no podrán reproducirse correctamente.")
    else:
        logger.info("Archivo de audio de despedida (goodbye.mp3) disponible.")

async def generate_audio_file(text, audio_path):
    """Genera un archivo de audio para un texto dado."""
    logger.info(f"Generando audio para mensaje estático: {text}")
    try:
        audio_generated_path = await generate_audio_async(text, audio_path)
        if audio_generated_path:
            logger.info(f"Audio estático generado y guardado en: {audio_generated_path}")
        else:
            logger.error("Error al generar el audio estático.")
    except Exception as e:
        logger.error(f"Error al generar audio estático: {e}", exc_info=True)

@app.on_event("startup")
async def startup_event():
    await generate_static_audio()

def is_call_active(call_sid):
    try:
        call = twilio_client.calls(call_sid).fetch()
        logger.debug(f"Estado de la llamada: {call.status}")
        return call.status in ["in-progress", "ringing"]
    except Exception as e:
        logger.error(f"Verificando estado de la llamada: {e}", exc_info=True)
        return False

def process_audio(audio_queue, stop_event, call_context):
    """Procesa el audio entrante usando Google Speech-to-Text."""
    try:
        speech_client = speech.SpeechClient()
        recognition_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.MULAW,
            sample_rate_hertz=8000,
            language_code="en-US",
            enable_automatic_punctuation=True,
            max_alternatives=1,
            speech_contexts=[speech.SpeechContext(phrases=[], boost=15.0)],
        )
        streaming_config = speech.StreamingRecognitionConfig(
            config=recognition_config,
            interim_results=False,
            single_utterance=False,
        )

        def audio_generator():
            buffer = bytearray()
            last_audio_time = time.time()
            while not stop_event.is_set():
                try:
                    audio_content = audio_queue.get(timeout=0.005)
                    buffer.extend(audio_content)
                    last_audio_time = time.time()
                    if len(buffer) >= 320:
                        yield speech.StreamingRecognizeRequest(audio_content=bytes(buffer))
                        logger.debug(f"Audio enviado al reconocedor: {len(buffer)} bytes")
                        buffer = bytearray()
                except queue.Empty:
                    if (time.time() - last_audio_time) >= 0.03:
                        # Enviar silencio si no hay audio
                        logger.debug("Enviando silencio para mantener la conexión.")
                        silence = b"\xFF" * 320
                        yield speech.StreamingRecognizeRequest(audio_content=silence)
                        last_audio_time = time.time()

        requests_generator = audio_generator()
        responses = speech_client.streaming_recognize(streaming_config, requests_generator)

        # Ejecutar la función asincrónica handle_responses_async
        asyncio.run(handle_responses_async(responses, call_context, stop_event))
    except Exception as e:
        logger.error(f"Procesando audio en tiempo real: {e}", exc_info=True)

# Lista de frases irrelevantes
irrelevant_phrases = [
    "thank you", "ok", "great", "alright", "cool",
    "uh-huh", "i'll wait", "i see", "um", "hmm", "thanks","okay","uh huh"
]

def is_irrelevant(transcript):
    """
    Determina si el transcript es irrelevante.
    Primero normalizamos el transcript: removemos puntuación, lo pasamos a minúsculas.
    Si el transcript normalizado está en la lista de frases irrelevantes, retornamos True.
    """
    normalized = transcript.strip().lower()
    normalized = normalized.translate(str.maketrans('', '', string.punctuation))
    normalized = " ".join(normalized.split())

    if normalized in irrelevant_phrases:
        return True
    return False

# (3) Función para detectar frases de despedida
def is_farewell(transcript):
    """
    Determina si el transcript es una despedida.
    Igual que en is_irrelevant, normalizamos y comparamos con la lista farewell_phrases.
    Esto evita que frases como 'good evening' activen la despedida, ya que no están en la lista.
    """
    normalized = transcript.strip().lower()
    normalized = normalized.translate(str.maketrans('', '', string.punctuation))
    normalized = " ".join(normalized.split())

    if normalized in farewell_phrases:
        return True
    return False

async def handle_responses_async(responses, call_context, stop_event):
    try:
        # Estado para controlar si el mensaje "wait" ya fue enviado
        wait_message_sent = False
        for response in responses:
            if not response.results:
                continue
            for result in response.results:
                if result.is_final:
                    transcript = result.alternatives[0].transcript.strip()

                    # Ignorar transcripciones vacías o muy cortas
                    if not transcript or len(transcript) < 3:
                        logger.info("Transcripción vacía o muy corta. Ignorando.")
                        continue

                    # Verificación temprana de irrelevancia
                    if is_irrelevant(transcript):
                        logger.info(f"Transcripción irrelevante detectada: '{transcript}'. Ignorando.")
                        continue

                    # (4) Verificación de despedida
                    if is_farewell(transcript):
                        logger.info(f"Transcripción detectada como despedida: '{transcript}'. Reproduciendo goodbye y cerrando la interacción.")
                        goodbye_audio_path = os.path.join(STATIC_AUDIO_PATH, "goodbye.mp3")
                        if os.path.exists(goodbye_audio_path):
                            # Reproducir mensaje de despedida
                            play_audio_to_caller(call_context["call_sid"], goodbye_audio_path)
                            # Detener la interacción (stop_event)
                            stop_event.set()
                            break
                        else:
                            logger.error("Archivo goodbye.mp3 no encontrado. No se puede reproducir despedida.")
                            # Aún así detenemos la interacción, ya que el usuario se despidió
                            stop_event.set()
                            break

                    logger.info(f"Transcripción Final: {transcript}")

                    # Esperar antes de responder para no interrumpir al usuario
                    await asyncio.sleep(2)

                    # Si no se ha enviado un mensaje "wait" todavía, enviar uno al usuario
                    if not wait_message_sent:
                        wait_phrases = STATIC_MESSAGES['wait']
                        selected_phrase = random.choice(wait_phrases)
                        idx = wait_phrases.index(selected_phrase)
                        wait_audio_filename = f"wait_{idx}.mp3"
                        wait_audio_path = os.path.join(STATIC_AUDIO_PATH, wait_audio_filename)
                        if os.path.exists(wait_audio_path):
                            play_audio_to_caller(call_context["call_sid"], wait_audio_path)
                            wait_message_sent = True
                        else:
                            logger.error(f"El archivo de audio estático para 'wait' no existe: {wait_audio_path}")

                    # Actualizar el historial de conversación con el mensaje del usuario
                    call_context["conversation_history"].append({"role": "user", "content": transcript})

                    # Limitar el historial si excede el límite de tokens
                    call_context["conversation_history"] = await trim_conversation_history(
                        call_context["conversation_history"]
                    )

                    # Obtener respuesta de OpenAI
                    openai_task = asyncio.create_task(
                        get_openai_response_async(call_context["conversation_history"])
                    )
                    openai_response = await openai_task

                    # Agregar la respuesta del asistente al historial
                    call_context["conversation_history"].append({"role": "assistant", "content": openai_response})

                    # Generar audio de la respuesta
                    audio_task = asyncio.create_task(generate_audio_async(openai_response))
                    response_audio_path = await audio_task

                    # Reproducir el audio generado
                    if response_audio_path:
                        play_audio_to_caller(call_context["call_sid"], response_audio_path)
                    else:
                        logger.error("No se pudo generar audio.")
                        if os.path.exists(ERROR_AUDIO_PATH):
                            play_audio_to_caller(call_context["call_sid"], ERROR_AUDIO_PATH)
                        else:
                            logger.error("El archivo de audio de error predeterminado no existe.")

                    # Reiniciar el estado para la siguiente interacción
                    wait_message_sent = False

                    # Si se solicita detener, salir del bucle
                    if stop_event.is_set():
                        break
    except Exception as e:
        logger.error(f"Manejo de respuestas asincrónicas: {e}", exc_info=True)

async def trim_conversation_history(conversation_history, max_tokens=4000):
    """Recorta el historial de conversación para no exceder el límite de tokens."""
    total_tokens = sum(len(message["content"].split()) for message in conversation_history)
    while total_tokens > max_tokens:
        # Eliminar el mensaje más antiguo (excepto el del sistema)
        if len(conversation_history) > 1:
            removed_message = conversation_history.pop(1)
            logger.debug(f"Eliminando mensaje antiguo: {removed_message}")
            total_tokens = sum(len(message["content"].split()) for message in conversation_history)
        else:
            break
    return conversation_history

async def get_openai_response_async(conversation_history):
    try:
        logger.info("Generando respuesta con OpenAI para el historial actualizado.")
        loop = asyncio.get_event_loop()
        start_time = time.time()
        response = await loop.run_in_executor(
            None,
            lambda: openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=conversation_history,
            ),
        )
        end_time = time.time()
        logger.info(f"Tiempo en obtener respuesta de OpenAI: {end_time - start_time:.2f} segundos")
        answer = response["choices"][0]["message"]["content"].strip()
        if not answer:
            logger.error("Respuesta de OpenAI vacía.")
            return "I'm sorry, I couldn't process your request."
        logger.info(f"Respuesta generada por OpenAI: {answer}")
        return answer
    except Exception as e:
        logger.error(f"Generando respuesta con OpenAI: {e}", exc_info=True)
        return "I'm sorry, I couldn't process your request."

def split_text(text, max_length):
    """Divide el texto en fragmentos que no excedan max_length caracteres."""
    words = text.split()
    chunks = []
    current_chunk = ""
    for word in words:
        if len(current_chunk) + len(word) + 1 <= max_length:
            current_chunk += (" " + word) if current_chunk else word
        else:
            chunks.append(current_chunk)
            current_chunk = word
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def concatenate_audio_files(audio_files, output_path):
    """Combina múltiples archivos de audio en uno solo."""
    try:
        combined = AudioSegment.empty()
        for file in audio_files:
            audio_segment = AudioSegment.from_file(file)
            combined += audio_segment
        combined.export(output_path, format="mp3")
        logger.info(f"Archivos de audio concatenados y guardados en: {output_path}")
        # Eliminar los archivos de audio temporales
        for file in audio_files:
            os.remove(file)
            logger.debug(f"Archivo temporal eliminado: {file}")
        return output_path
    except Exception as e:
        logger.error(f"Error al concatenar archivos de audio: {e}", exc_info=True)
        return None

async def generate_audio_async(text, audio_path=None):
    try:
        if not text or len(text.strip()) < 5:
            logger.error("Texto para generar audio está vacío o es demasiado corto.")
            return None
        logger.info(f"Generando audio con Eleven Labs para el texto: {text}")

        # Limite preventivo de caracteres
        text = text[:5000]
        text_chunks = split_text(text, MAX_CHAR_LIMIT)
        logger.debug(f"Texto dividido en {len(text_chunks)} fragmentos.")

        audio_files = []
        for idx, chunk in enumerate(text_chunks):
            logger.debug(f"Generando audio para el fragmento {idx + 1}: {chunk}")
            success = False
            attempts = 0
            while not success and attempts < MAX_RETRIES:
                attempts += 1
                try:
                    url = f"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}"
                    headers = {
                        "Accept": "audio/mpeg",
                        "Content-Type": "application/json",
                        "xi-api-key": ELEVENLABS_API_KEY,
                    }
                    data = {
                        "text": chunk,
                        "voice_settings": {
                            "stability": ELEVENLABS_STABILITY,
                            "similarity_boost": ELEVENLABS_SIMILARITY_BOOST,
                        },
                    }

                    async with httpx.AsyncClient() as client:
                        start_time = time.time()
                        response = await client.post(url, json=data, headers=headers)
                        end_time = time.time()
                        logger.info(f"Intento {attempts}: Tiempo en generar audio para fragmento {idx + 1}: {end_time - start_time:.2f} segundos")

                        if response.status_code == 200:
                            if not os.path.exists("static"):
                                os.makedirs("static")
                            timestamp = int(time.time() * 1000)
                            temp_audio_filename = f"Response_{timestamp}_{idx}.mp3"
                            temp_audio_path = os.path.join("static", temp_audio_filename)
                            with open(temp_audio_path, "wb") as f:
                                f.write(response.content)
                            logger.info(f"Audio del fragmento {idx + 1} guardado: {temp_audio_path}")
                            audio_files.append(temp_audio_path)
                            success = True
                        else:
                            logger.error(f"Error en fragmento {idx + 1}, intento {attempts}: {response.text}")
                            if attempts >= MAX_RETRIES:
                                logger.error(f"No se pudo generar audio para el fragmento {idx + 1} después de {MAX_RETRIES} intentos.")
                    if not success and attempts >= MAX_RETRIES:
                        break
                except Exception as e:
                    logger.error(f"Excepción al generar audio para fragmento {idx + 1}, intento {attempts}: {e}", exc_info=True)
                    if attempts >= MAX_RETRIES:
                        logger.error(f"No se pudo generar audio para el fragmento {idx + 1} después de {MAX_RETRIES} intentos.")
            if not success:
                break

        if len(audio_files) == len(text_chunks):
            if not audio_files:
                logger.error("No se generaron archivos de audio.")
                return None
            if audio_path is None:
                timestamp = int(time.time())
                audio_filename = f"Response_{timestamp}.mp3"
                audio_path = os.path.join("static", audio_filename)
            final_audio_path = concatenate_audio_files(audio_files, audio_path)
            if final_audio_path:
                logger.info(f"Audio final generado y guardado: {final_audio_path}")
                return final_audio_path
            else:
                logger.error("No se pudo concatenar los archivos de audio.")
                return None
        else:
            logger.error("No se pudieron generar todos los fragmentos de audio.")
            if os.path.exists(ERROR_AUDIO_PATH):
                logger.info(f"Usando el archivo de audio de error predeterminado: {ERROR_AUDIO_PATH}")
                return ERROR_AUDIO_PATH
            else:
                logger.error("El archivo de audio de error predeterminado no existe.")
                return None

    except Exception as e:
        logger.error(f"Generando audio asincrónicamente: {e}", exc_info=True)
        return None

def play_audio_to_caller(call_sid, audio_path):
    try:
        if not call_sid:
            logger.error("call_sid es None, no se puede reproducir el audio.")
            return

        if not is_call_active(call_sid):
            logger.error("La llamada no está activa, no se puede reproducir el audio.")
            return

        if not os.path.exists(audio_path):
            logger.error(f"El archivo de audio no existe: {audio_path}")
            if os.path.exists(ERROR_AUDIO_PATH):
                logger.info(f"Usando el archivo de audio de error predeterminado: {ERROR_AUDIO_PATH}")
                audio_path = ERROR_AUDIO_PATH
            else:
                logger.error("El archivo de audio de error predeterminado no existe. No se puede reproducir audio.")
                return

        audio_url = f"https://{NGROK_URL}/static/{os.path.basename(audio_path)}"
        logger.info(f"URL del audio: {audio_url}")

        response = VoiceResponse()
        response.play(audio_url)
        # Se incluye una pausa y un redirect para mantener la llamada y permitir más interacciones
        response.pause(length=60)
        response.redirect(TWIML_BIN_URL)
        logger.debug(f"TwiML generado: {str(response)}")

        twilio_client.calls(call_sid).update(twiml=str(response))
        logger.info("Audio reproducido correctamente al llamante.")
    except Exception as e:
        logger.error(f"Reproduciendo audio: {e}", exc_info=True)

@app.get("/static/{filename}")
async def serve_static(filename: str):
    return FileResponse(path=f"static/{filename}")

@app.websocket("/media")
async def media_socket(websocket: WebSocket):
    logger.info("Conexión WebSocket establecida.")
    await websocket.accept()
    audio_queue = queue.Queue()
    stop_event = threading.Event()
    # Historial inicial de la conversación
    call_context = {
        "call_sid": None,
        "conversation_history": [
            {
                "role": "system",
                "content": (
                    "You are Jessica, a customer service representative for NetConnect Internet Services. "
                    "Your role is to assist with inquiries related to services, billing, technical support, "
                    "and general information. Follow the provided business guidelines strictly."
                ),
            }
        ],
    }

    # Iniciar el hilo de procesamiento de audio
    threading.Thread(
        target=process_audio, args=(audio_queue, stop_event, call_context), daemon=True
    ).start()

    try:
        while not stop_event.is_set():
            try:
                message = await websocket.receive_text()
                if message is None:
                    logger.info("Conexión WebSocket cerrada por el cliente.")
                    break
                event_data = json.loads(message)
                event = event_data.get("event", "")
                logger.debug(f"Evento recibido: {event}")

                if event == "start":
                    call_context["call_sid"] = event_data["start"]["callSid"]
                    logger.info(f"Call SID recibido: {call_context['call_sid']}")

                    # Reproducir mensaje de bienvenida
                    welcome_audio_path = os.path.join(STATIC_AUDIO_PATH, "welcome.mp3")
                    if os.path.exists(welcome_audio_path):
                        play_audio_to_caller(call_context["call_sid"], welcome_audio_path)
                    else:
                        logger.error("El archivo de audio estático para 'welcome' no existe.")

                elif event == "media":
                    payload = event_data["media"]["payload"]
                    audio_content = base64.b64decode(payload)
                    audio_queue.put(audio_content)
                    logger.debug(f"Audio recibido y agregado a la cola: {len(audio_content)} bytes")
                elif event == "stop":
                    logger.info("Evento 'stop' recibido. Cerrando conexión.")
                    stop_event.set()  # Asegurar que el hilo de audio se detenga
                    break
                else:
                    logger.warning(f"Evento no manejado: {event}")
            except Exception as e:
                logger.error(f"En WebSocket: {e}", exc_info=True)
                break
    except WebSocketDisconnect:
        logger.info("WebSocket desconectado.")
    finally:
        stop_event.set()
        await websocket.close()
        logger.info("WebSocket cerrado.")

if __name__ == "__main__":
    try:
        logger.info("Iniciando servidor...")
        uvicorn.run(app, host="0.0.0.0", port=8080)
    except Exception as e:
        logger.error(f"Error General: {str(e)}", exc_info=True)
        